Introduction

Day 1-> Big data overview and processes
Day 2-> batch, Scala intro
Day 3-> Stream 
Day 4-> advanced batch and stream process/time windows
Day 5 -> AI and ML 



Structured-> SQL (MySQL, Postgre, MSSQL, Oracle) , neatly arranged in folders/sub-folders by date_time_stamp
							-> every datapoint commit to a schema
	single video-> structured images in an array, fixed FPS, fixed audio playback!

Semi-structure -> graph, XML, JSON, images, Excel/CSV/TSV

Unstructured-> blob, images, text, emails, group of videos


2 types of architectures:
Lambda
	-> 2 separate processes- one for Batch, one for Stream (legacy systems), or if the processing of batch and stream was significantly different

Kappa
	-> UNIFIED architecture (batch & stream), modern cost-saving architecture, the processing of batch and stream needs to be same!
		-> speed layer, batch layer



OLTP v/s OLAP Databases

OLTP- Online transactional process
							- No/SQL DB -MySQL, SQL, Postgre, MongoDB, Graph ql
							- website, mobile apps, software
							- upto TB
							- 1st, 2nd, 3rd normal form for optimisation-> storage optimisation -> ACID principles -> JOIN operations
							- DB is normalised
					-> Great for editing, poor for searching 

OLAP - Online Analytical process 
							- SQL DW - Synapse, BigQuery, BigTable, Apache Hive, HBase
						- Data analytics, ML, AI
						-> From TB to PB+
						-> JOIN ops are worst performing OPS!!!
					-> we stay away from the db normalisation!
				-> instead, we go with 1 big FLAT table!
					-> DW is denormalised!
				-> great for searching, poor performance on editing the data



Big data is data too big for 1 machine 
1 machine = 2 numbers 

If I ask you to store 1,2,3 -> is this big data or small data?

(Fault tolerance) [Almost all OLTP]
M1-> 1,2			—replication— M3-> 1,2
M2-> 3																M4-> 3

GCP-> Google Colossus 

Sharding (Fault tolerance) [All OLAP]

M1-> 1,2			M2-> 2,3			M3-> 3,1 


Big data-> great for searching, bad for editing
Small data-> great for editing, bad for searching

Searching:

OLAP- M1-> 1,2			M2-> 2,3			M3-> 3,1 
OLTP-> M1-> 1,2.   M2-> 2,3

We want to search for 3, every operation takes 1ms

OLTP-> 

M1-> 1  2
M2-> 2 3
1st msecond-> M1 (1), M2(2)-> not found 
2nd msecond-> M1(2), M2(3) -> found 

Total time-> 2 milliseconds!

OLAP-> 

M1-> 1,2
M2-> 2,3
M3-> 3,1

1st ms -> M1(1), M2(2), M3(3) -> Found!
Total time-> 1ms!


Editing is BAD in OLAP!
New problem statement: we want to change 2 to 4 

OLTP-> 

M1-> 1  2
M2-> 2 3

1st ms -> M1 (1), M2 (2)
2nd ms -> M1 (2), M2 (editing)
3rd ms-> M1 (editing), M2 (3)
4th ms-> end of search
—— over——— (changes sent to replica as well)—— 
Total cost-> 8ms

OLAP-> 

M1-> 1,2
M2-> 2,3
M3-> 3,1

1st ms -> M1(1), M2(2), M3(3)
2nd ms-> M1(2), M2(editing), M3(1)
3rd ms-> M1(editing), M2(3), ——
4th ms-> ——, ——, ——

Total cost-> 12ms

Even the slightest edits mean that you have to SCAN the entire warehouse!!!

3 types of data engineering activities:
(Common for DA or ML or DS….)

Extract-Load (EL) : 
	1. Data is clean and no more changes
	2. DIRECTLY use the data
	3. Example: loading into Power BI!
	4. Mostly individual or small teams

Extract-Transform-Load (ETL):
	1. Data requires cleaning 
	2. MISSION SPECIFIC- you need to know your entire data transformation journey before building the pipeline-> and its specific only to 1 task!
	3. ML pipelines, data cleaning pipelines, Kubernetes pipelines, data migration pipelines
	4. Cost effective for small teams
	5. An ETL pipeline could talk to other ETL pipelines to form an ELT pipeline!

Extract-Load-Transform (ELT):
	1. Data may or may not require cleaning
	2. Objective is to get ALL the data into cloud first to avoid any latency going forward and have a SINGLE SOURCE OF TRUTH
	3. Used when we have multiple missions from the same data-> example-> ML, DA and DE activities required on the same dataset!
	4. Cost effective for LARGE teams
	5. COLLECT EVERYTHING INTO THE CLOUD; and THEN FIGURE OUT WHAT TO DO NEXT!
	6. Medical dataset-> HR dept wants to find out which patients need funding support, hospital management wants to find out which customers are more likely to book premium rooms; a doctor may want to look up records of a particular patient and share it with anonymisation with other doctors!
	7. An ELT pipeline consists of multiple ETL pipelines


The individual pipelines are usually ETL
Multiple pipelines with STORAGE first-> ELT

ETL-> 
	1. Extract from on-premise Oracle DB
	2. Change datatypes 
	3. Load into on-premise Microsoft SQL Server


	1. Extract from sensors 
	2. Add timestamps 
	3. Load into a no-sql database 

	1. Extract from file
	2. Write a python/SQL program to make changes
	3. Write into a new file





ELT->

	1. Extract inventory, billing from multiple stores 
	2. Load into Hadoop 
	3. 
		A)Using Spark figure out which inventory needs reshipping (ETL1)
		B) Using Power BI to figure out top 10 selling products (EL1)
		C) Using Hadoop to build a real time fraud detection process (no movement)


Data Factory: 
1. Linked services are dedicated network services 
2. Datasets are a POINTER SCHEMA REPRESENTATION of data, not the ACTUAL data! -> schema, network details, how to connect


Lab 1: https://learn.microsoft.com/en-us/azure/azure-sql/database/single-database-create-quickstart


Lab 2: Create Data Factory and Storage Account
 
Both cases-> please select the same Resource Group Name we created for SQL DB, and enter a globally unique name for the resource name

other settings will remain at default (ONLY Resource Group name and resource name to be selected in both cases)

Lab 3: Build pipeline to copy data in Data Factory
https://learn.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-portal

Step 1: (Manage on left menu) Create Linked Service
 
Step 2: (Author on left menu) Create datasets. Publish changes so that datasets are not lost on refresh. 
 
Step 3: (Author on left menu) Create pipeline 





















